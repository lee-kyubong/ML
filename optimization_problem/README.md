## 1. Gradient descent
- 임의의 지점에서 시작해 경사를 따라 내려갈 수 없을 때까지 반복적으로 내려가며 최적화 수행
- 초기지점 theta 값에서의 경사값을 구함 / 경사를 따라 eta만큼 내려간다(경사값의 -1 방향으로 eta만큼 움직인다) / <br/>
손실함수의 출력값이 많이 줄었나 보고 처음 과정으로 돌아가 계속 하강. 많이 줄어들지 않았다면 exit
- 경사값은 loss fn을 theta에 대해 미분
- 얼마나 빨리내려갈지 learning rate, or step size는 eta로 결정. 학습률이 크면 매 스텝마다 theta를 많이 변화시켜서 빠른 시간에 학습되기는 하지만,
수렴되지못해 계속 왔다 갔다하는 불안정한 모습을 보이기도 한다. 학습률이 작다면 시간이 오래 걸리지만 최솟값 근처에 안정적으로 수렴. 여러가지 학습률을 시도해보고 학습률 조절(대-소)
## 2. Stochastic gradient descent
- 일부 데이터만 이용해 손실함수와 1차 미분값을 근사적으로 계산하는 확률적 경사하강법. 전체 데이터를 계산하기보단 n개의 샘플로 손실함수와 1차 미분값 계산(sweep, mini batch)
- 미니 배치의 크기 n과 학습률 eta를 다양하게 시도해보고 적절한 값 선택이 중요.
- 배치의 크기나 학습률의 초기 설정에 민감한 SGD 성질에 대해 최근 보완 연구 진행 중(Adam: 과거 미분값의 방향과 분산을 계속 가중평균 내며 효율적인 업데이트 방향과 크기 선택)
