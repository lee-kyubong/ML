## 회귀와 분류 문제에 적용 가능한 의사결정 트리
- 설명변수의 공간을 다수의 영역으로 계층화(stratifying) 또는 분할(segmenting)한다.
- 주어진 관측치에 대한 예측을 하기 위해서는 보통 그 관측치가 속하는 영역의 훈련 관측치들의 평균 또는 최빈값을 사용한다.
- 설명변수 공간을 분할하는데 사용되는 분할규칙들은 트리로 요약될 수 있으므로, 이러한 유형의 기법들은 decision tree라 칭한다.
- 트리 기반 방법들은 해석하기 쉽고 유옹하다. 허나 예측 정확도가 떨어져 배깅, 랜덤 포레스트, 부스팅과 같은 다중트리 활용기법 필요. 허나 해석력이 떨어질 수 있다.
- 회귀 step_1: 설명변수 공간 X1, X2, Xp에 대한 가능한 값들의 집합을, J개의 다르고 겹치지 않는 영역 R1, R2, Rj로 분할
- 회귀 step_2: 영역 Rj에 속하는 모든 관측치들에 대해 동일한 예측을 하며, 예측값은 Rj의 훈련 관측치들에 대한 반응변수 값들의 평균
- 영역 R1, Rj 는 결과 예측모델이 간단하고 해석하기 쉽도록 *설명변수공간*을 고차원의 직사각형 또는 박스로 분할.
- sum(yi - y har Rj)^2 이 RSS를 최소로하는 박스 R1, Rj를 찾는 것이 목적!
- 설명변수공간을 j개의 박스로 분할하는 모든 가능한 경우를 다 고려하는 것은 계산상 불가능. 때문에 recursive binary spliting의 top-down의 greedy 기법 사용.
- 하향식: 트리의 맨 위(모든 관측치들이 단일영역에 속하는 점)에서 시작해 예측공간을 연속해서 분할해 내려가는 방식. 이 때, 각 분할은 트리에서 아래 방향으로 생성된 두 개의 새로운 가지로 표시.
- greedy: 트리를 만드는 과정의 각 단계에서 미리 앞을 내다보고 나중에 나오는 어떤 단계에서 더 나은 트리가 될 분할을 선택하는 것이 아니라,  *그 특정 단계에서 가장 좋은 분할을 선택*
- 재귀이진분할 수행은 먼저 설명변수 Xj의 절단점 s를 선택. 이 절단점 s는 RSS가 가능한 가장 작게 되도록 설명변수 공간을 분할. 즉, 모든 설명변수 X1, Xp와 각 설명변수에 대해 절단점 s의 모든 가능한 값을 고려하고, 그 다음에 RSS가 가장 작은 트리를 가져다 주는 설명변수와 절단점을 선택.
- 다음으로 가장 좋은 설명변수와 절단점을 찾는 과정을 반복하여 각 영역 내에서 RSS가 최소가 되도록 데이터를 더 분할. 이제 영역은 3개. 다시 이 세 영역 중 하나를 RSS가 최소가 되게 더 분할. 이 과정을 Stopping criterion이 만족될 때 까지 지속. 예를들어, 어떠한 영역도 5개보다 많은 관측치를 포함하지 않을 때까지 계속. 영역 R1, Rj가 만들어지고 나면, 주어진 검정 관측치에 대한 반응변수는 그 검정 관측치가 속하는 영역 내 훈련 관측치들의 평균 사용해 예측
- 단점: 과적합의 가능성으로 검정셋 성능 나쁨. 분산이 높다 편향은 작지만.
- 분할 수가 적은 더 작은 트리(즉 Rj의 수가 더 적은)는 약간의 편향 증가가 있지만 분산이 더 낮아지고 해석이 쉽다.(과적합 모델은 민감, 즉 편향은 작더라도 분산이 커 새로운 input에 대한 예측값이 몹시 불안정) 위 과정에 대한 한 가지 대안은 각각의 분할로 인한 RSS 감소가 어떤 높은 임계값을 초과하는 동안까지만 트리빌딩 과정을 진행(이 역시 단편적 해결. 트리빌딩 초기에 쓸모없어 보이는 분할 이후에 아주 좋은 분할, 즉 나중에 RSS값을 크게 줄이는 분할이 올 수 있기 때문)
-  더 나은 전략은 아주 큰 트리 T0를 만들고, 그것을 다시 prune하여 subtree 얻는 것. 교차검증으로 가능한 서브트리들의 순위 결정. lasso처럼 RSS + alpha|T|
- 분류에서는 RSS 대신 분류오류율 활용(노드가 pure하도록 지니 지수(node purity), 교차엔트로피 활용)
- 분류 트리에서 관측치에 대한 예측된 반응변수 값은, 각 관측치가 그것이 속하는 영역 내 훈련 관측치들이 가장 많이 포함된 클래스에 속하는지를 예측
- 배깅(부트스트랩 어그리게이션), 랜포, 부스팅
- 트리기반 모델은 예측 데이터의 특성에 영향을 받지 않는다.(하지만 선형은 아니다.) 피처 엔지니어링(비율화 등)는 회귀에는 중요할 수 있다 허나 트리 같은 어떤모델은 내부적으로 feature selection을 갖고 있어서 이 모델에는 정확도를 극대화하는 예측 변수만 포함된다.
- 이상치에 강건하다 SVM과 함께.
- 결측값을 특이하게 처리 후 모델링 가능
